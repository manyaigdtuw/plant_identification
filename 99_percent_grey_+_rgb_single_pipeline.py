# -*- coding: utf-8 -*-
"""99 percent grey + rgb single pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13z-wtppkol9A03VqmxTywKLAHUrh4Uxd
"""

import shutil
import os

drive_path = "/content/drive"
if os.path.ismount(drive_path):
    print("Drive is already mounted.")
else:
    print("Removing stale mountpoint files...")
    shutil.rmtree(drive_path, ignore_errors=True)

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

data_dir = '/content/drive/My Drive/MedicinalLeavesProject/data/Indian Medicinal Leaves Image Datasets/Medicinal Leaf dataset'

labels = sorted([name for name in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, name))])
print("Labels (Plant/Leaf Names):")
for label in labels:
    print(label)
print(f"\nTotal labels: {len(labels)}")

import os
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.models import Model
import numpy as np
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard

# Configuration <3
data_dir = '/content/drive/My Drive/MedicinalLeavesProject/data/Indian Medicinal Leaves Image Datasets/Medicinal Leaf dataset'
img_size = 224
batch_size = 32
base_learning_rate = 1e-4
epochs = 30


# Preprocessing and Data Augmentation bbg
def preprocess(img):
    return tf.keras.applications.mobilenet_v2.preprocess_input(img)

def random_grayscale(img):
    if tf.random.uniform(()) < 0.3:
        img = tf.image.rgb_to_grayscale(img)
        img = tf.image.grayscale_to_rgb(img)
    return img

def augmentation_fn(img):
    img = tf.image.random_flip_left_right(img)
    img = tf.image.random_flip_up_down(img)
    img = tf.image.random_brightness(img, max_delta=0.2)
    img = tf.image.random_saturation(img, lower=0.8, upper=1.2)
    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)
    img = tf.image.rot90(img, k)
    img = random_grayscale(img)
    return img

def decode_img(img_bytes):
    img = tf.io.decode_image(img_bytes, channels=3, expand_animations=False)
    img = tf.image.resize(img, [img_size, img_size])
    img = tf.cast(img, tf.float32)
    return img

def load_and_preprocess(image_path, label):
    img_bytes = tf.io.read_file(image_path)
    img = decode_img(img_bytes)
    img = augmentation_fn(img)
    img = preprocess(img)
    return img, label

def load_and_preprocess_no_aug(image_path, label):
    img_bytes = tf.io.read_file(image_path)
    img = decode_img(img_bytes)
    img = preprocess(img)
    return img, label

# Preparing Dataset
def get_dataset(data_dir, batch_size, augment=False, shuffle=True, shuffle_buffer=1000):
    AUTOTUNE = tf.data.AUTOTUNE
    list_ds = tf.data.Dataset.list_files(str(data_dir + '/*/*'), shuffle=shuffle)

    # Filter valid image files
    list_ds = list_ds.filter(lambda x: tf.strings.regex_full_match(x, ".*(jpg|jpeg|png|bmp|gif|JPG|JPEG|PNG|BMP|GIF)"))

    # class names are basically folder names okay ? okay.
    class_names = np.array(sorted([item.name for item in os.scandir(data_dir) if item.is_dir()]))

    def get_label(file_path):
        parts = tf.strings.split(file_path, os.path.sep)
        label = parts[-2]  # folder name
        return tf.squeeze(tf.where(class_names == label)[0])

    labeled_ds = list_ds.map(lambda x: (x, get_label(x)), num_parallel_calls=AUTOTUNE)

    if augment:
        ds = labeled_ds.map(load_and_preprocess, num_parallel_calls=AUTOTUNE)
    else:
        ds = labeled_ds.map(load_and_preprocess_no_aug, num_parallel_calls=AUTOTUNE)

    if shuffle:
        ds = ds.shuffle(shuffle_buffer)
    ds = ds.batch(batch_size)
    ds = ds.prefetch(buffer_size=AUTOTUNE)
    return ds, class_names

# Setup datasets

train_ds, class_names = get_dataset(data_dir, batch_size, augment=True)
val_ds, _ = get_dataset(data_dir, batch_size, augment=False)
test_ds, _ = get_dataset(data_dir, batch_size, augment=False)

num_classes = len(class_names)
print("Classes:", class_names)

# Model <33
input_tensor = Input(shape=(img_size, img_size, 3))
base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_tensor)

for layer in base_model.layers:
    layer.trainable = False
for layer in base_model.layers[-30:]:
    layer.trainable = True

x = GlobalAveragePooling2D()(base_model.output)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.4)(x)
x = Dense(128, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)
predictions = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=predictions)

model.compile(
    optimizer=Adam(learning_rate=base_learning_rate),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Callbacks used to prevent overfitting and stuff here it will stop after 8 similar attempts :)

early_stopping = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_medicinal_leaf_mobilenetv2.h5', monitor='val_accuracy', save_best_only=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)
tensorboard_callback = TensorBoard(log_dir='./logs')

callback_list = [early_stopping, model_checkpoint, reduce_lr, tensorboard_callback]

# training starts here

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs,
    callbacks=callback_list,
    verbose=1
)

# Evaluation <3
test_loss, test_acc = model.evaluate(test_ds)
print(f'Test accuracy: {test_acc:.3f}')

import sys
print(sys.version)

import tensorflow as tf
print(tf.__version__)

model.save('/content/drive/My Drive/MedicinalLeavesProject/highestaccuracy17sept.h5')
model.save('/content/drive/My Drive/MedicinalLeavesProject/highestaccuracy17sept.keras')

import gradio as gr
import tensorflow as tf
import numpy as np
from tensorflow.keras.models import load_model
from PIL import Image

# Load trained model (update path if you save as .keras instead of .h5)
model = tf.keras.models.load_model('/content/drive/My Drive/MedicinalLeavesProject/highestaccuracy17sept.h5')
data_dir = '/content/drive/My Drive/MedicinalLeavesProject/data/Indian Medicinal Leaves Image Datasets/Medicinal Leaf dataset'

# Make sure to keep class_names consistent
class_names = np.array(sorted([item.name for item in os.scandir(data_dir) if item.is_dir()]))

img_size = 224

def preprocess_image(img):
    img = img.convert("RGB")  # Ensure 3 channels
    img = img.resize((img_size, img_size))
    img_array = np.array(img).astype("float32")
    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)
    return np.expand_dims(img_array, axis=0)

def predict(image):
    processed = preprocess_image(image)
    preds = model.predict(processed)[0]
    confidences = {class_names[i]: float(preds[i]) for i in range(len(class_names))}
    top_class = class_names[np.argmax(preds)]
    return f"Prediction: {top_class}", confidences

demo = gr.Interface(
    fn=predict,
    inputs=gr.Image(type="pil", label="Upload a Leaf Image"),
    outputs=[
        gr.Textbox(label="Top Prediction"),
        gr.Label(label="Class Probabilities")
    ],
    title="Medicinal Leaf Classifier",
    description="Upload an image of a leaf and the model will predict its medicinal category."
)

if __name__ == "__main__":
    demo.launch(share=True)